{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9VGSD3qAUou"
      },
      "source": [
        "###**Encoder-only Transformer vs. LSTM + Attention for Text Classification**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Tl9JmQyBoL0"
      },
      "source": [
        "###**AG News Dataset**\n",
        "The AG News dataset is a collection of over one million news articles categorized into four classes:\n",
        "\n",
        "World\n",
        "\n",
        "Sports\n",
        "\n",
        "Business\n",
        "\n",
        "Science/Technology\n",
        "\n",
        "For this implementation, we'll use the version provided by the torchtext library, which includes:\n",
        "\n",
        "Training set: 120,000 samples (30,000 per class)\n",
        "\n",
        "Test set: 7,600 samples (1,900 per class)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "Jy5nUQ-CG3MM",
        "outputId": "1ed90600-7165-4ee9-bfcc-423737c15bc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.3.0 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.0.1+cu118 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "1909e4400fbf484fa5bca986e8b82a63",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install numpy==1.24.4 --force-reinstall"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRhi7GdrDYkM",
        "outputId": "ec5afd4f-579b-4145-ff61-8d2d3ca78978"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch==2.0.1+cu118 in /usr/local/lib/python3.11/dist-packages (2.0.1+cu118)\n",
            "Requirement already satisfied: torchtext==0.15.2 in /usr/local/lib/python3.11/dist-packages (0.15.2+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (4.13.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (3.1.6)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.0.1+cu118) (2.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (2.32.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (2.2.4)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /usr/local/lib/python3.11/dist-packages (from torchtext==0.15.2) (0.6.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.11/dist-packages (from torchdata==0.6.1->torchtext==0.15.2) (2.4.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (3.31.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.11/dist-packages (from triton==2.0.0->torch==2.0.1+cu118) (18.1.8)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.0.1+cu118) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torchtext==0.15.2) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.0.1+cu118) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch==2.0.1+cu118 torchtext==0.15.2 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANGOk7hcEbnv",
        "outputId": "62928917-9c0e-4669-ba34-fd3d11ffe9be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Torch Version: 2.0.1+cu118\n",
            "TorchText Version: 0.15.2+cpu\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "\n",
        "print(\"Torch Version:\", torch.__version__)\n",
        "print(\"TorchText Version:\", torchtext.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "4KeMoXGtFHAP",
        "outputId": "cbddbb78-902e-4f0b-c32d-339574a0505c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: portalocker 2.7.0\n",
            "Uninstalling portalocker-2.7.0:\n",
            "  Successfully uninstalled portalocker-2.7.0\n",
            "Collecting portalocker==2.7.0\n",
            "  Using cached portalocker-2.7.0-py2.py3-none-any.whl.metadata (6.8 kB)\n",
            "Using cached portalocker-2.7.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: portalocker\n",
            "Successfully installed portalocker-2.7.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "7be14ee7a10941f9a973ec9a5a89d4f5",
              "pip_warning": {
                "packages": [
                  "portalocker"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip uninstall portalocker -y\n",
        "!pip install portalocker==2.7.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqqXay15DjCq"
      },
      "source": [
        "####**Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "09XTV-_2ChQm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import portalocker\n",
        "from torchtext.datasets import AG_NEWS\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cgDSXJYhDKpJ"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "\n",
        "# Build vocabulary\n",
        "def yield_tokens(data_iter):\n",
        "    for label, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])\n",
        "\n",
        "# Define text and label pipelines\n",
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3ZC_SmJF26F"
      },
      "source": [
        "####**DataLoader Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V1Zpz1LsFBG-"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list = [], []\n",
        "    for (_label, _text) in batch:\n",
        "        label_list.append(label_pipeline(_label))\n",
        "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "        text_list.append(processed_text)\n",
        "    text_list = pad_sequence(text_list, batch_first=True)\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    return text_list, label_list\n",
        "\n",
        "# Create DataLoaders\n",
        "from torch.utils.data import IterableDataset\n",
        "\n",
        "class AGNewsDataset(IterableDataset):\n",
        "    def __init__(self, data_iter):\n",
        "        self.data_iter = list(data_iter)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.data_iter)\n",
        "\n",
        "train_dataset = AGNewsDataset(AG_NEWS(split='train'))\n",
        "test_dataset = AGNewsDataset(AG_NEWS(split='test'))\n",
        "\n",
        "batch_size = 64\n",
        "# Remove shuffle=True for train_dataloader as it's not supported with IterableDataset\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, collate_fn=collate_batch) # Change here\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW9u7olyGKUp"
      },
      "source": [
        "###**Model Implementations**\n",
        "\n",
        "####**1. Encoder-only Transformer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "RAIiGjVuF0Cs"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, num_heads, hidden_dim, num_layers, num_classes, max_len=512):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, max_len, embed_dim))\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=hidden_dim)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.fc = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_len = x.size(1)\n",
        "        x = self.embedding(x) + self.pos_embedding[:, :seq_len, :]\n",
        "        x = x.permute(1, 0, 2)  # Transformer expects input of shape (seq_len, batch_size, embed_dim)\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.mean(dim=0)  # Global average pooling\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2H6mDssGVi7"
      },
      "source": [
        "####**2. LSTM with Attention**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Kemyw7bHGIOl"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        self.attn = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_weights = torch.tanh(self.attn(x))\n",
        "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
        "        context = torch.sum(attn_weights * x, dim=1)\n",
        "        return context\n",
        "\n",
        "class LSTMWithAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_classes):\n",
        "        super(LSTMWithAttention, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
        "        self.attention = Attention(hidden_dim * 2)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "        attn_out = self.attention(lstm_out)\n",
        "        return self.fc(attn_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88MJrDqTGcWP"
      },
      "source": [
        "####**Training and Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "tR5Ww5kDGTWU"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def train(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    for text, labels in dataloader:\n",
        "        text, labels = text.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(text)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_acc += (output.argmax(1) == labels).sum().item()\n",
        "        total_count += labels.size(0)\n",
        "    return total_acc / total_count\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for text, labels in dataloader:\n",
        "            text, labels = text.to(device), labels.to(device)\n",
        "            output = model(text)\n",
        "            loss = criterion(output, labels)\n",
        "            total_acc += (output.argmax(1) == labels).sum().item()\n",
        "            total_count += labels.size(0)\n",
        "    return total_acc / total_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "J8wmckf2GaEg"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 128\n",
        "hidden_dim = 256\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "num_classes = 4\n",
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(vocab)\n",
        "embed_dim = 128\n",
        "hidden_dim = 256\n",
        "num_heads = 4\n",
        "num_layers = 2\n",
        "num_classes = 4\n",
        "epochs = 5\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Initialize models\n",
        "transformer_model = TransformerClassifier(vocab_size, embed_dim, num_heads, hidden_dim, num_layers, num_classes).to(device)\n",
        "lstm_model = LSTMWithAttention(vocab_size, embed_dim, hidden_dim, num_classes).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Cq4WXiPMGils"
      },
      "outputs": [],
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "transformer_optimizer = optim.Adam(transformer_model.parameters(), lr=learning_rate)\n",
        "lstm_optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b88csWMaIFD0",
        "outputId": "d5cf2af5-12bc-46b3-a98c-04adfef6fb33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Transformer Model\n",
            "\n",
            "Training LSTM with Attention Model\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.8967105263157895"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Training Transformer Model\")\n",
        "train(transformer_model, train_dataloader, criterion, transformer_optimizer, device)\n",
        "\n",
        "print(\"\\nTraining LSTM with Attention Model\")\n",
        "train(lstm_model, train_dataloader, criterion, lstm_optimizer, device)\n",
        "evaluate(lstm_model, test_dataloader, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uO56gkST7ru"
      },
      "source": [
        "###**Complexity Comparison**\n",
        "\n",
        "**Transformer (Encoder Only):**  \n",
        "\n",
        "---\n",
        "1: Time Complexity\t O(n² * d) due to self-attention\n",
        "\n",
        "2: Space Complexity\tO(n²) for attention matrix\n",
        "\n",
        "3: Parallelization\tHigh (parallelizable across sequence positions)\n",
        "\n",
        "4: Scalability\tBetter for long sequences\n",
        "\n",
        "**LSTM + Attention**\n",
        "\n",
        "1: O(n * d²) where d is hidden size\n",
        "\n",
        "2: O(n * d) for hidden states\n",
        "\n",
        "3: Low (sequential computation)\n",
        "\n",
        "4: Slower with increasing sequence length\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eveZJbzeVA7r"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "###**Language Understanding Capabilities**\n",
        "\n",
        "**Transformer:**\n",
        "\n",
        "1: Captures global dependencies regardless of token distance.\n",
        "\n",
        "2: Better at understanding contextual relationships.\n",
        "\n",
        "3: More parallelizable and scalable.\n",
        "\n",
        "**LSTM + Attention:**\n",
        "\n",
        "1: Captures temporal patterns well.\n",
        "\n",
        "2: Attention helps with long-term dependencies but still limited by sequential LSTM structure.\n",
        "\n",
        "3: Struggles with longer contexts and vanishing gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PQrn5VDVYSL"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Note:**\n",
        "\n",
        "Use Transformer when you have enough data and compute power — great for long sequences and global context.\n",
        "\n",
        "Use LSTM + Attention when working with shorter texts or on resource-constrained devices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0yGuNjBlIK69"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
